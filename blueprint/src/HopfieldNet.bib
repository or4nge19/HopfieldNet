@InProceedings{lean4,
author="Moura, Leonardo de
and Ullrich, Sebastian",
editor="Platzer, Andr{\'e}
and Sutcliffe, Geoff",
title="The {L}ean 4 {T}heorem {P}rover and {P}rogramming {L}anguage",
booktitle="Automated Deduction -- CADE 28",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="625--635",
abstract="Lean 4 is a reimplementation of the Lean interactive theorem prover (ITP) in Lean itself. It addresses many shortcomings of the previous versions and contains many new features. Lean 4 is fully extensible: users can modify and extend the parser, elaborator, tactics, decision procedures, pretty printer, and code generator. The new system has a hygienic macro system custom-built for ITPs. It contains a new typeclass resolution procedure based on tabled resolution, addressing significant performance problems reported by the growing user base. Lean 4 is also an efficient functional programming language based on a novel programming paradigm called functional but in-place. Efficient code generation is crucial for Lean users because many write custom proof automation procedures in Lean itself.",
isbn="978-3-030-79876-5"
}

@misc{modern,
    title={{H}opfield {N}etworks is {A}ll {Y}ou {N}eed}, 
      author={Hubert Ramsauer and Bernhard Schäfl and Johannes Lehner and Philipp Seidl and Michael Widrich and Thomas Adler and Lukas Gruber and Markus Holzleitner and Milena Pavlović and Geir Kjetil Sandve and Victor Greiff and David Kreil and Michael Kopp and Günter Klambauer and Johannes Brandstetter and Sepp Hochreiter},
      year={2021},
      eprint={2008.02217},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/2008.02217}, 
}

@InProceedings{liu,
author="Liu, Liya
and Aravantinos, Vincent
and Hasan, Osman
and Tahar, Sofi{\`e}ne",
editor="Merz, Stephan
and Pang, Jun",
title="{O}n the {F}ormal {A}nalysis of {HMM} {U}sing {T}heorem {P}roving",
booktitle="Formal Methods and Software Engineering",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="316--331",
abstract="Hidden Markov Models (HMMs) have been widely utilized for modeling time series data in various engineering and biological systems. The analyses of these models are usually conducted using computer simulations and paper-and-pencil proof methods and, more recently, using probabilistic model-checking. However, all these methods either do not guarantee accurate analysis or are not scalable (for instance, they can hardly handle the computation when some parameters become very huge). As an alternative, we propose to use higher-order logic theorem proving to reason about properties of discrete HMMs by applying automated verification techniques. This paper presents some foundational formalizations in this regard, namely an extended-real numbers based formalization of finite-state Discrete-Time Markov chains and HMMs along with the verification of some of their fundamental properties. The distinguishing feature of our work is that it facilitates automatic verification of systems involving HMMs. For illustration purposes, we utilize our results for the formal analysis of a DNA sequence.",
isbn="978-3-319-11737-9"
}

@book{baran,
  title={{N}eural {C}omputation in {H}opfield {N}etworks and {B}oltzmann {M}achines},
  author={James P. Coughlin and Robert H. Baran},
  year={1995},
  url={https://api.semanticscholar.org/CorpusID:59952015},
  publisher = {Rowman \& Littlefield}
}

@misc{SciLean,
  author = {Tomáš Skřivan},
  title = {{S}ciLean: {S}cientific {C}omputing in {L}ean},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/lecopivo/SciLean}},
}

@inproceedings{pwa,
author = {Aleksandrov, Andrei and V\"{o}llinger, Kim},
title = {{F}ormalizing {P}iecewise {A}ffine {A}ctivation {F}unctions of {N}eural {N}etworks in {C}oq},
year = {2023},
isbn = {978-3-031-33169-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-33170-1_4},
doi = {10.1007/978-3-031-33170-1_4},
abstract = {Verification of neural networks relies on activation functions being piecewise affine (pwa)—enabling an encoding of the verification problem for theorem provers. In this paper, we present the first formalization of pwa activation functions for an interactive theorem prover tailored to verifying neural networks within Coq using the library Coquelicot for real analysis. As a proof-of-concept, we construct the popular pwa activation function ReLU. We integrate our formalization into a Coq model of neural networks, and devise a verified transformation from a neural network N to a pwa function representing N by composing pwa functions that we construct for each layer. This representation enables encodings for proof automation, e.g. Coq ’s tactic lra – a decision procedure for linear real arithmetic. Further, our formalization paves the way for integrating Coq in frameworks of neural network verification as a fallback prover when automated proving fails.},
booktitle = {NASA Formal Methods: 15th International Symposium, NFM 2023, Houston, TX, USA, May 16–18, 2023, Proceedings},
pages = {62–78},
numpages = {17},
keywords = {Verification, Coq, Interactive Theorem Prover, Neural Network, Piecewise Affine Function},
location = {Houston, TX, USA}
}

@article{OPT-035,
url = {http://dx.doi.org/10.1561/2400000035},
year = {2021},
volume = {4},
journal = {Foundations and Trends in Optimization},
title = {{A}lgorithms for {V}erifying {D}eep {N}eural {N}etworks},
doi = {10.1561/2400000035},
issn = {2167-3888},
number = {3-4},
pages = {244-404},
author = {Changliu Liu and Tomer Arnon and Christopher Lazarus and Christopher Strong and Clark Barrett and Mykel J. Kochenderfer}
}

@INPROCEEDINGS{8953865,
  author={Lin, Wang and Yang, Zhengfeng and Chen, Xin and Zhao, Qingye and Li, Xiangkun and Liu, Zhiming and He, Jifeng},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={{R}obustness {V}erification of {C}lassification {D}eep {N}eural {N}etworks via {L}inear {P}rogramming}, 
  year={2019},
  volume={},
  number={},
  pages={11410-11419},
  keywords={Deep Learning},
  doi={10.1109/CVPR.2019.01168}}


@InProceedings{10.1007/978-3-030-53288-8_2,
author="Tran, Hoang-Dung
and Bak, Stanley
and Xiang, Weiming
and Johnson, Taylor T.",
editor="Lahiri, Shuvendu K.
and Wang, Chao",
title={{V}erification of {D}eep {C}onvolutional {N}eural {N}etworks {U}sing {I}mageStars},
booktitle="Computer Aided Verification",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="18--42",
abstract="Convolutional Neural Networks (CNN) have redefined state-of-the-art in many real-world applications, such as facial recognition, image classification, human pose estimation, and semantic segmentation. Despite their success, CNNs are vulnerable to adversarial attacks, where slight changes to their inputs may lead to sharp changes in their output in even well-trained networks. Set-based analysis methods can detect or prove the absence of bounded adversarial attacks, which can then be used to evaluate the effectiveness of neural network training methodology. Unfortunately, existing verification approaches have limited scalability in terms of the size of networks that can be analyzed. In this paper, we describe a set-based framework that successfully deals with real-world CNNs, such as VGG16 and VGG19, that have high accuracy on ImageNet. Our approach is based on a new set representation called the ImageStar, which enables efficient exact and over-approximative analysis of CNNs. ImageStars perform efficient set-based analysis by combining operations on concrete images with linear programming (LP). Our approach is implemented in a tool called NNV, and can verify the robustness of VGG networks with respect to a small set of input states, derived from adversarial attacks, such as the DeepFool attack. The experimental results show that our approach is less conservative and faster than existing zonotope and polytope methods.",
isbn="978-3-030-53288-8"
}


@InProceedings{10.1007/978-3-319-68167-2_19,
author="Ehlers, R{\"u}diger",
editor="D'Souza, Deepak
and Narayan Kumar, K.",
title={{F}ormal {V}erification of {P}iece-{W}ise {L}inear {F}eed-{F}orward {N}eural {N}etworks},
booktitle="Automated Technology for Verification and Analysis",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="269--286",
abstract="We present an approach for the verification of feed-forward neural networks in which all nodes have a piece-wise linear activation function. Such networks are often used in deep learning and have been shown to be hard to verify for modern satisfiability modulo theory (SMT) and integer linear programming (ILP) solvers.",
isbn="978-3-319-68167-2"
}


@inproceedings{NEURIPS2018_be53d253,
 author = {Bunel, Rudy R and Turkaslan, Ilker and Torr, Philip and Kohli, Pushmeet and Mudigonda, Pawan K},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {{A} {U}nified {V}iew of {P}iecewise {L}inear {N}eural {N}etwork {V}erification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/be53d253d6bc3258a8160556dda3e9b2-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{Botoeva_Kouvaros_Kronqvist_Lomuscio_Misener_2020, 
title={{E}fficient {V}erification of {R}e{LU}-{B}ased {N}eural {N}etworks via {D}ependency {A}nalysis}, 
volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/5729}, 
DOI={10.1609/aaai.v34i04.5729},
 abstractNote={&lt;p&gt;We introduce an efficient method for the verification of ReLU-based 
 feed-forward neural networks. We derive an automated procedure that exploits dependency 
 relations between the ReLU nodes, thereby pruning the search tree that needs to be considered 
 by MILP-based formulations of the verification problem. We augment the resulting algorithm with 
 methods for input domain splitting and symbolic interval propagation. We present Venus, the 
 resulting verification toolkit, and evaluate it on the ACAS collision avoidance networks and
  models trained on the MNIST and CIFAR-10 datasets. The experimental results obtained indicate
   considerable gains over the present state-of-the-art tools.&lt;/p&gt;}, 
   number={04}, journal={Proceedings of the AAAI Conference on Artificial Intelligence},
    author={Botoeva, Elena and Kouvaros, Panagiotis and Kronqvist, Jan and Lomuscio,
     Alessio and Misener, Ruth}, year={2020}, month={Apr.}, pages={3291-3299} }

@book{comp,
  title={{C}omputational {I}ntelligence},
  author={Kruse, Rudolf and Borgelt, Christian and Braune, 
  Christian and Mostaghim, Sanaz and Steinbrecher, Matthias and Klawonn, Frank and Moewes, Christian},
  year={2011},
  publisher={Springer}}

@inproceedings{mathlib2020lean,
  title={{The {L}ean mathematical library}},
  author={mathlib Community.},
  booktitle={Proceedings of the 9th ACM SIGPLAN International 
  Conference on Certified Programs and Proofs (CPP 2020)},
  pages={367--381},
  year={2020}
}

@article{bentkamp2019formal,
  title={{A} {F}ormal {P}roof of the {E}xpressiveness of {D}eep {L}earning},
  author={Bentkamp, Alexander and Blanchette, Jasmin Christian and Klakow, Dietrich},
  journal={Journal of Automated Reasoning},
  volume={63},
  pages={347--368},
  year={2019},
  publisher={Springer}
}


@article{hinton,
title = {{A} {L}earning {A}lgorithm for {B}oltzmann {M}achines},
journal = {Cognitive Science},
volume = {9},
number = {1},
pages = {147-169},
year = {1985},
issn = {0364-0213},
doi = {https://doi.org/10.1016/S0364-0213(85)80012-4},
url = {https://www.sciencedirect.com/science/article/pii/S0364021385800124},
author = {David H. Ackley and Geoffrey E. Hinton and Terrence J. Sejnowski},
abstract = {The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure.}
}

@ARTICLE{geman,
  author={Geman, Stuart and Geman, Donald},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={{S}tochastic {R}elaxation, {G}ibbs {D}istributions, and the {B}ayesian {R}estoration of {I}mages}, 
  year={1984},
  volume={PAMI-6},
  number={6},
  pages={721-741},
  keywords={Stochastic processes;Bayesian methods;Image restoration;Degradation;Markov random fields;Additive noise;Deformable models;Temperature distribution;Energy states;Annealing;Annealing;Gibbs distribution;image restoration;line process;MAP estimate;Markov random field;relaxation;scene modeling;spatial degradation},
  doi={10.1109/TPAMI.1984.4767596}}

@article{hinton2,
  title={{L}earning and {R}elearning in {B}oltzmann {M}achines},
  author={Hinton, Geoffrey E and Sejnowski, Terrence J and others},
  journal={Parallel distributed processing: Explorations in the microstructure of cognition},
  volume={1},
  number={282-317},
  pages={2},
  year={1986}
}


@book{levin,
  title={{M}arkov {C}hains and {M}ixing {T}imes},
  author={Levin, David A and Peres, Yuval},
  volume={107},
  year={2017},
  publisher={American Mathematical Soc.}
}

@misc{britt,
  author    = {Lean Community},
  title     = {{C}omputational {N}euroscience in {L}ean?},
  year      = {2024},
  month     = {September},
  howpublished = {\url{https://leanprover.zulipchat.com/#narrow/channel/113488-general/topic/Computational.20Neuroscience.20in.20Lean.3F}},
  note      = {Accessed: 2025-03-24}
}

@article{tso,
doi = {10.1209/0295-5075/6/2/002},
url = {https://dx.doi.org/10.1209/0295-5075/6/2/002},
year = {1988},
month = {may},
publisher = {},
volume = {6},
number = {2},
pages = {101},
author = {M. V. Tsodyks and M. V. Feigel'man},
title = {{T}he {E}nhanced {S}torage {C}apacity in {N}eural {N}etworks with {L}ow {A}ctivity {L}evel},
journal = {Europhysics Letters},
abstract = {The modified Hopfield model defined in terms of “V-variables” (V = 0; 1), which is appropriate for storage of correlated patterns, is considered. The learning algorithm is proposed to enhance significantly the storage capacity in comparison with previous estimates. At low levels of neural activity, p ≪ 1, we obtain αc(p) ∼ (p|ln p|)-1 which resembles Gardner's estimate for the maximum storage capacity.}
}

@article{van,
  title = {{N}onlinear {N}eural {N}etworks},
  author = {van Hemmen, J. L. and K\"uhn, R.},
  journal = {Phys. Rev. Lett.},
  volume = {57},
  issue = {7},
  pages = {913--916},
  numpages = {0},
  year = {1986},
  month = {Aug},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.57.913},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.57.913}
}


@article{parisi,
doi = {10.1088/0305-4470/19/11/005},
url = {https://dx.doi.org/10.1088/0305-4470/19/11/005},
year = {1986},
month = {aug},
publisher = {},
volume = {19},
number = {11},
pages = {L675},
author = {G Parisi},
title = {{A}symmetric {N}eural {N}etworks and the {P}rocess of {L}earning},
journal = {Journal of Physics A: Mathematical and General},
abstract = {Studies the influence of a strong asymmetry of the synaptic strengths on the behavior of a neural network which works as an associative memory. The author finds that the asymmetry in the synaptic strengths may be crucial for the process of learning.}
}

@article{physlean,
    author = "Tooby-Smith, Joseph",
    title = "{HepLean: Digitalising {H}igh {E}nergy {P}hysics}",
    eprint = "2405.08863",
    archivePrefix = "arXiv",
    primaryClass = "hep-ph",
    doi = "10.1016/j.cpc.2024.109457",
    journal = "Comput. Phys. Commun.",
    volume = "308",
    pages = "109457",
    year = "2025"
}


@article{
kirk,
author = {S. Kirkpatrick  and C. D. Gelatt  and M. P. Vecchi },
title = {{O}ptimization by {S}imulated {A}nnealing},
journal = {Science},
volume = {220},
number = {4598},
pages = {671-680},
year = {1983},
doi = {10.1126/science.220.4598.671},
URL = {https://www.science.org/doi/abs/10.1126/science.220.4598.671},
eprint = {https://www.science.org/doi/pdf/10.1126/science.220.4598.671},
abstract = {There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.}}


@article{metropolis,
    author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
    title = {{E}quation of {S}tate {C}alculations by {F}ast {C}omputing {M}achines},
    journal = {The Journal of Chemical Physics},
    volume = {21},
    number = {6},
    pages = {1087-1092},
    year = {1953},
    month = {06},
    abstract = {A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two‐dimensional rigid‐sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four‐term virial coefficient expansion.},
    issn = {0021-9606},
    doi = {10.1063/1.1699114},
    url = {https://doi.org/10.1063/1.1699114},
    eprint = {https://pubs.aip.org/aip/jcp/article-pdf/21/6/1087/18802390/1087\_1\_online.pdf},
}


@article{hastings,
    author = {Hastings, W. K.},
    title = {{M}onte {C}arlo {S}ampling {M}ethods {U}sing {M}arkov {C}hains and {T}heir {A}pplications},
    journal = {Biometrika},
    volume = {57},
    number = {1},
    pages = {97-109},
    year = {1970},
    month = {04},
    abstract = {A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.},
    issn = {0006-3444},
    doi = {10.1093/biomet/57.1.97},
    url = {https://doi.org/10.1093/biomet/57.1.97},
    eprint = {https://academic.oup.com/biomet/article-pdf/57/1/97/23940249/57-1-97.pdf},
}

@book{amit,
  title={Modeling {B}rain {F}unction: {T}he {W}orld of {A}ttractor {N}eural {N}etworks},
  author={Amit, Daniel J and Amit, Daniel J},
  year={1989},
  publisher={Cambridge university press}
}

@article{hertz,
    author = {Hertz, John and Krogh, Anders and Palmer, Richard G. and Horner, Heinz},
    title = {{I}ntroduction to the {T}heory of {N}eural {C}omputation},
    journal = {Physics Today},
    volume = {44},
    number = {12},
    pages = {70-70},
    year = {1991},
    month = {12},
    issn = {0031-9228},
    doi = {10.1063/1.2810360},
    url = {https://doi.org/10.1063/1.2810360},
    eprint = {https://pubs.aip.org/physicstoday/article-pdf/44/12/70/7510527/70\_1\_online.pdf},
}



@article{storage,
title = {{O}ptimization of {S}tochastic {N}networks {U}sing {S}imulated {A}nnealing for the {S}torage and {R}ecalling of {C}ompressed {I}mages {U}sing {SOM}},
journal = {Engineering Applications of Artificial Intelligence},
volume = {26},
number = {10},
pages = {2383-2396},
year = {2013},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2013.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0952197613001310},
author = {Manu Pratap Singh and Rinku Sharma Dixit},
keywords = {Hopfield neural network, SOM, Stochastic networks, Simulated annealing, FFT, DWT},
abstract = {In this paper we are studying the optimization of Stochastic Hopfield neural network and the hybrid SOM–Hopfield neural network for the storage and recalling of fingerprint images. The feature extraction of these images has been performed using FFT, DWT and SOM. The feature vectors are stored in the Hopfield network with Hebbian learning and modified Pseudoinverse learning rules. The study explores the tolerance of Hopfield neural networks for reducing the effect of spurious minima in the recalling process by employing the Simulated annealing process. It is observed from the simulations that the capabilities of the Hopfield network can be sufficiently enhanced by making modifications in the feature extraction of the input data. DWT and SOM together can be used to significantly enhance the recall efficiency. The probability of error in recall in the form of spurious minima is minimized by adopting simulated annealing process in the pattern recalling process.}
}

@phdthesis{bhat2013syntactic,
  title={{S}yntactic {F}oundations for {M}achine {L}earning.},
  author={Bhat, Sooraj},
  year={2013},
  school={Georgia Institute of Technology}
}

@article{Ising:1925em,
    author = "Ising, Ernst",
    title = "{{C}ontribution to the {T}heory of {F}erromagnetism}",
    doi = "10.1007/BF02980577",
    journal = "Z. Phys.",
    volume = "31",
    pages = "253--258",
    year = "1925"
}



@inproceedings{selsam,
author = {Selsam, Daniel and Liang, Percy and Dill, David L.},
title = {{D}eveloping {B}ug-{F}ree {M}achine {L}earning {S}ystems with {F}ormal {M}athematics},
year = {2017},
publisher = {JMLR.org},
abstract = {Noisy data, non-convex objectives, model misspecification, and numerical instability can all cause undesired behaviors in machine learning systems. As a result, detecting actual implementation errors can be extremely difficult. We demonstrate a methodology in which developers use an interactive proof assistant to both implement their system and to state a formal theorem defining what it means for their system to be correct. The process of proving this theorem interactively in the proof assistant exposes all implementation errors since any error in the program would cause the proof to fail. As a case study, we implement a new system, Certigrad, for optimizing over stochastic computation graphs, and we generate a formal (i.e. machine-checkable) proof that the gradients sampled by the system are unbiased estimates of the true mathematical gradients. We train a variational autoencoder using Certigrad and find the performance comparable to training the same model in TensorFlow.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3047–3056},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{murphy2017verified,
  title={{V}erified {P}erceptron {C}onvergence {T}heorem},
  author={Murphy, Charlie and Gray, Patrick and Stewart, Gordon},
  booktitle={Proceedings of the 1st ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
  pages={43--50},
  year={2017}
}

@article{Bagnall_Stewart_2019, title={{C}ertifying the {T}rue {E}rror: {M}achine {L}earning in {C}oq with {V}erified {G}eneralization {G}uarantees}, volume={33}, url={https://ojs.aaai.org/index.php/AAAI/article/view/4115}, DOI={10.1609/aaai.v33i01.33012662}, abstractNote={&lt;p&gt;We present MLCERT, a novel system for doing practical mechanized proof of the generalization of learning procedures, bounding expected error in terms of training or test error. MLCERT is mechanized in that we prove generalization bounds inside the theorem prover Coq; thus the bounds are machine checked by Coq’s proof checker. MLCERT is practical in that we extract learning procedures defined in Coq to executable code; thus procedures with proved generalization bounds can be trained and deployed in real systems. MLCERT is well documented and open source; thus we expect it to be usable even by those without Coq expertise. To validate MLCERT, which is compatible with external tools such as TensorFlow, we use it to prove generalization bounds on neural networks trained using TensorFlow on the extended MNIST data set.&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Bagnall, Alexander and Stewart, Gordon}, year={2019}, month={Jul.}, pages={2662-2669} }

@inproceedings{10.1007/978-3-031-27481-7_24,
author = {Brucker, Achim D. and Stell, Amy},
title = {{V}erifying {F}eedforward {N}eural {N}etworks for {C}lassification in {I}sabelle/{HOL}},
year = {2023},
isbn = {978-3-031-27480-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-27481-7_24},
doi = {10.1007/978-3-031-27481-7_24},
abstract = {Neural networks are being used successfully to solve classification problems, e.g., for detecting objects in images. It is well known that neural networks are susceptible if small changes applied to their input result in misclassification. Situations in which such a slight input change, often hardly noticeable by a human expert, results in a misclassification are called adversarial examples. If such inputs are used for adversarial attacks, they can be life-threatening if, for example, they occur in image classification systems used in autonomous cars or medical diagnosis.Systems employing neural networks, e.g., for safety or security-critical functionality, are a particular challenge for formal verification, which usually expects a formal specification (e.g., given as source code in a programming language for which a formal semantics exists). Such a formal specification does, per se, not exist for neural networks.In this paper, we address this challenge by presenting a formal embedding of feedforward neural networks into Isabelle/HOL and discussing desirable properties for neural networks in critical applications. Our Isabelle-based prototype can import neural networks trained in TensorFlow, and we demonstrate our approach using a neural network trained for the classification of digits on a dot-matrix display.},
booktitle = {Formal Methods: 25th International Symposium, FM 2023, L\"{u}beck, Germany, March 6–10, 2023, Proceedings},
pages = {427–444},
numpages = {18},
keywords = {Isabelle/HOL, Verification, Feedforward network, Classification network, Deep learning, Neural network},
location = {L\"{u}beck, Germany}
}


@MISC{nobel,
  title        = "{T}he {N}obel {P}rize in {P}hysics 2024",
  booktitle    = "{NobelPrize.org}",
  abstract     = "The Nobel Prize in Physics 2024 was awarded jointly to John
                  J. Hopfield and Geoffrey E. Hinton ``for foundational
                  discoveries and inventions that enable machine learning with
                  artificial neural networks''",
  howpublished = "\url{https://www.nobelprize.org/prizes/physics/2024/press-release/}",
  note         = "Accessed: 2025-1-5",
  language     = "en"
}


@article{hopfield1982neural,
  title={{N}eural {N}etworks and {P}hysical {S}ystems with {E}mergent {C}ollective {C}omputational {A}bilities},
  author={Hopfield, John J},
  journal={Proceedings of the national academy of sciences},
  volume={79},
  number={8},
  pages={2554--2558},
  year={1982},
  publisher={National Acad Sciences}
}
